apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: asgd-test
  namespace: ray-jobs
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics"
    prometheus.io/port: "8080"
spec:
  # ASGD test with 2 GPUs (1 head + 1 worker) - Testing Prometheus metrics
  entrypoint: python -u /app/main.py --mode asgd --num-workers 2 --total-updates 5000 --num-samples 500000 --noise 0.005 --dim 100 --lr 0.005 --batch 32768 --device cuda
  shutdownAfterJobFinishes: true
  ttlSecondsAfterFinished: 1800
  runtimeEnvYAML: |
    env_vars:
      PYTHONPATH: "/app"
      # NCCL debugging and metrics
      NCCL_DEBUG: "INFO"
      NCCL_DEBUG_SUBSYS: "INIT,COLL,P2P,NET"
      # NCCL network configuration
      NCCL_IB_DISABLE: "1"
      NCCL_SOCKET_IFNAME: "eth0"
      # PyTorch distributed backend
      TORCH_DISTRIBUTED_DEBUG: "INFO"
  rayClusterSpec:
    rayVersion: "2.53.0"
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
        metrics-export-port: "8080"
        num-gpus: "1"
      template:
        metadata:
          annotations:
            karpenter.sh/do-not-disrupt: "true"
        spec:
          # Tolerate GPU node taints
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
          # Force scheduling on GPU NodePool
          nodeSelector:
            karpenter.sh/nodepool: gpu
          containers:
            - name: ray-head
              image: 496105080108.dkr.ecr.us-east-2.amazonaws.com/ddp:latest
              imagePullPolicy: Always
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 52365
                  name: api
                - containerPort: 8080
                  name: metrics
              resources:
                limits:
                  cpu: "2"
                  memory: "8Gi"
                  nvidia.com/gpu: "1"
                requests:
                  cpu: "500m"
                  memory: "4Gi"
                  nvidia.com/gpu: "1"
    workerGroupSpecs:
      # REDUCED for 8 vCPU quota: only 1 worker pod (+ 1 head = 2 GPUs total)
      - replicas: 1
        minReplicas: 1
        maxReplicas: 1
        groupName: worker-group
        rayStartParams:
          metrics-export-port: "8080"
          num-gpus: "1"
        template:
          spec:
            # Tolerate GPU node taints
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            # Force scheduling on GPU NodePool
            nodeSelector:
              karpenter.sh/nodepool: gpu
            containers:
              - name: ray-worker
                image: 496105080108.dkr.ecr.us-east-2.amazonaws.com/ddp:latest
                imagePullPolicy: Always
                ports:
                  - containerPort: 6379
                    name: gcs-server
                  - containerPort: 8265
                    name: dashboard
                  - containerPort: 10001
                    name: client
                  - containerPort: 52365
                    name: api
                  - containerPort: 8080
                    name: metrics
                resources:
                  limits:
                    cpu: "2"
                    memory: "8Gi"
                    nvidia.com/gpu: "1"
                  requests:
                    cpu: "500m"
                    memory: "4Gi"
                    nvidia.com/gpu: "1"
